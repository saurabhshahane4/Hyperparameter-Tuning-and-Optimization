{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Regularization.ipynb","provenance":[],"authorship_tag":"ABX9TyNhEq3v7jmyGFJoQLHlwTvM"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KwXp8f8CX8-_","colab_type":"text"},"source":["## Predifined Functions\n"]},{"cell_type":"code","metadata":{"id":"KoOwu1MiW4W4","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596450439568,"user_tz":-330,"elapsed":1465,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import h5py\n","import sklearn\n","import sklearn.datasets\n","import sklearn.linear_model\n","import scipy.io\n","\n","def sigmoid(x):\n","    \"\"\"\n","    Compute the sigmoid of x\n","\n","    Arguments:\n","    x -- A scalar or numpy array of any size.\n","\n","    Return:\n","    s -- sigmoid(x)\n","    \"\"\"\n","    s = 1/(1+np.exp(-x))\n","    return s\n","\n","def relu(x):\n","    \"\"\"\n","    Compute the relu of x\n","\n","    Arguments:\n","    x -- A scalar or numpy array of any size.\n","\n","    Return:\n","    s -- relu(x)\n","    \"\"\"\n","    s = np.maximum(0,x)\n","    \n","    return s\n","\n","def load_planar_dataset(seed):\n","    \n","    np.random.seed(seed)\n","    \n","    m = 400 # number of examples\n","    N = int(m/2) # number of points per class\n","    D = 2 # dimensionality\n","    X = np.zeros((m,D)) # data matrix where each row is a single example\n","    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n","    a = 4 # maximum ray of the flower\n","\n","    for j in range(2):\n","        ix = range(N*j,N*(j+1))\n","        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n","        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n","        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n","        Y[ix] = j\n","        \n","    X = X.T\n","    Y = Y.T\n","\n","    return X, Y\n","\n","def initialize_parameters(layer_dims):\n","    \"\"\"\n","    Arguments:\n","    layer_dims -- python array (list) containing the dimensions of each layer in our network\n","    \n","    Returns:\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n","                    W1 -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n","                    b1 -- bias vector of shape (layer_dims[l], 1)\n","                    Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n","                    bl -- bias vector of shape (1, layer_dims[l])\n","                    \n","    Tips:\n","    - For example: the layer_dims for the \"Planar Data classification model\" would have been [2,2,1]. \n","    This means W1's shape was (2,2), b1 was (1,2), W2 was (2,1) and b2 was (1,1). Now you have to generalize it!\n","    - In the for loop, use parameters['W' + str(l)] to access Wl, where l is the iterative integer.\n","    \"\"\"\n","    \n","    np.random.seed(3)\n","    parameters = {}\n","    L = len(layer_dims) # number of layers in the network\n","\n","    for l in range(1, L):\n","        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n","        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n","        \n","        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n","        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n","\n","        \n","    return parameters\n","\n","def forward_propagation(X, parameters):\n","    \"\"\"\n","    Implements the forward propagation (and computes the loss) presented in Figure 2.\n","    \n","    Arguments:\n","    X -- input dataset, of shape (input size, number of examples)\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n","                    W1 -- weight matrix of shape ()\n","                    b1 -- bias vector of shape ()\n","                    W2 -- weight matrix of shape ()\n","                    b2 -- bias vector of shape ()\n","                    W3 -- weight matrix of shape ()\n","                    b3 -- bias vector of shape ()\n","    \n","    Returns:\n","    loss -- the loss function (vanilla logistic loss)\n","    \"\"\"\n","        \n","    # retrieve parameters\n","    W1 = parameters[\"W1\"]\n","    b1 = parameters[\"b1\"]\n","    W2 = parameters[\"W2\"]\n","    b2 = parameters[\"b2\"]\n","    W3 = parameters[\"W3\"]\n","    b3 = parameters[\"b3\"]\n","    \n","    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n","    Z1 = np.dot(W1, X) + b1\n","    A1 = relu(Z1)\n","    Z2 = np.dot(W2, A1) + b2\n","    A2 = relu(Z2)\n","    Z3 = np.dot(W3, A2) + b3\n","    A3 = sigmoid(Z3)\n","    \n","    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n","    \n","    return A3, cache\n","\n","def backward_propagation(X, Y, cache):\n","    \"\"\"\n","    Implement the backward propagation presented in figure 2.\n","    \n","    Arguments:\n","    X -- input dataset, of shape (input size, number of examples)\n","    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat)\n","    cache -- cache output from forward_propagation()\n","    \n","    Returns:\n","    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n","    \"\"\"\n","    m = X.shape[1]\n","    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n","    \n","    dZ3 = A3 - Y\n","    dW3 = 1./m * np.dot(dZ3, A2.T)\n","    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n","    \n","    dA2 = np.dot(W3.T, dZ3)\n","    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n","    dW2 = 1./m * np.dot(dZ2, A1.T)\n","    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n","    \n","    dA1 = np.dot(W2.T, dZ2)\n","    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n","    dW1 = 1./m * np.dot(dZ1, X.T)\n","    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n","    \n","    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n","                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n","                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n","    \n","    return gradients\n","\n","def update_parameters(parameters, grads, learning_rate):\n","    \"\"\"\n","    Update parameters using gradient descent\n","    \n","    Arguments:\n","    parameters -- python dictionary containing your parameters:\n","                    parameters['W' + str(i)] = Wi\n","                    parameters['b' + str(i)] = bi\n","    grads -- python dictionary containing your gradients for each parameters:\n","                    grads['dW' + str(i)] = dWi\n","                    grads['db' + str(i)] = dbi\n","    learning_rate -- the learning rate, scalar.\n","    \n","    Returns:\n","    parameters -- python dictionary containing your updated parameters \n","    \"\"\"\n","    \n","    n = len(parameters) // 2 # number of layers in the neural networks\n","\n","    # Update rule for each parameter\n","    for k in range(n):\n","        parameters[\"W\" + str(k+1)] = parameters[\"W\" + str(k+1)] - learning_rate * grads[\"dW\" + str(k+1)]\n","        parameters[\"b\" + str(k+1)] = parameters[\"b\" + str(k+1)] - learning_rate * grads[\"db\" + str(k+1)]\n","        \n","    return parameters\n","\n","def predict(X, y, parameters):\n","    \"\"\"\n","    This function is used to predict the results of a  n-layer neural network.\n","    \n","    Arguments:\n","    X -- data set of examples you would like to label\n","    parameters -- parameters of the trained model\n","    \n","    Returns:\n","    p -- predictions for the given dataset X\n","    \"\"\"\n","    \n","    m = X.shape[1]\n","    p = np.zeros((1,m), dtype = np.int)\n","    \n","    # Forward propagation\n","    a3, caches = forward_propagation(X, parameters)\n","    \n","    # convert probas to 0/1 predictions\n","    for i in range(0, a3.shape[1]):\n","        if a3[0,i] > 0.5:\n","            p[0,i] = 1\n","        else:\n","            p[0,i] = 0\n","\n","    # print results\n","\n","    #print (\"predictions: \" + str(p[0,:]))\n","    #print (\"true labels: \" + str(y[0,:]))\n","    print(\"Accuracy: \"  + str(np.mean((p[0,:] == y[0,:]))))\n","    \n","    return p\n","\n","def compute_cost(a3, Y):\n","    \"\"\"\n","    Implement the cost function\n","    \n","    Arguments:\n","    a3 -- post-activation, output of forward propagation\n","    Y -- \"true\" labels vector, same shape as a3\n","    \n","    Returns:\n","    cost - value of the cost function\n","    \"\"\"\n","    m = Y.shape[1]\n","    \n","    logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)\n","    cost = 1./m * np.nansum(logprobs)\n","    \n","    return cost\n","\n","def load_dataset():\n","    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n","    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n","    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n","\n","    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n","    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n","    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n","\n","    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n","    \n","    train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n","    test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n","    \n","    train_set_x_orig = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n","    test_set_x_orig = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n","    \n","    train_set_x = train_set_x_orig/255\n","    test_set_x = test_set_x_orig/255\n","\n","    return train_set_x, train_set_y, test_set_x, test_set_y, classes\n","\n","\n","def predict_dec(parameters, X):\n","    \"\"\"\n","    Used for plotting decision boundary.\n","    \n","    Arguments:\n","    parameters -- python dictionary containing your parameters \n","    X -- input data of size (m, K)\n","    \n","    Returns\n","    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n","    \"\"\"\n","    \n","    # Predict using forward propagation and a classification threshold of 0.5\n","    a3, cache = forward_propagation(X, parameters)\n","    predictions = (a3>0.5)\n","    return predictions\n","\n","def load_planar_dataset(randomness, seed):\n","    \n","    np.random.seed(seed)\n","    \n","    m = 50\n","    N = int(m/2) # number of points per class\n","    D = 2 # dimensionality\n","    X = np.zeros((m,D)) # data matrix where each row is a single example\n","    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n","    a = 2 # maximum ray of the flower\n","\n","    for j in range(2):\n","        \n","        ix = range(N*j,N*(j+1))\n","        if j == 0:\n","            t = np.linspace(j, 4*3.1415*(j+1),N) #+ np.random.randn(N)*randomness # theta\n","            r = 0.3*np.square(t) + np.random.randn(N)*randomness # radius\n","        if j == 1:\n","            t = np.linspace(j, 2*3.1415*(j+1),N) #+ np.random.randn(N)*randomness # theta\n","            r = 0.2*np.square(t) + np.random.randn(N)*randomness # radius\n","            \n","        X[ix] = np.c_[r*np.cos(t), r*np.sin(t)]\n","        Y[ix] = j\n","        \n","    X = X.T\n","    Y = Y.T\n","\n","    return X, Y\n","\n","def plot_decision_boundary(model, X, y):\n","    # Set min and max values and give it some padding\n","    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n","    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n","    h = 0.01\n","    # Generate a grid of points with distance h between them\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","    # Predict the function value for the whole grid\n","    Z = model(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","    # Plot the contour and training examples\n","    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n","    plt.ylabel('x2')\n","    plt.xlabel('x1')\n","    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n","    plt.show()\n","    \n","def load_2D_dataset():\n","    data = scipy.io.loadmat('datasets/data.mat')\n","    train_X = data['X'].T\n","    train_Y = data['y'].T\n","    test_X = data['Xval'].T\n","    test_Y = data['yval'].T\n","\n","    plt.scatter(train_X[0, :], train_X[1, :], c=train_Y, s=40, cmap=plt.cm.Spectral);\n","    \n","    return train_X, train_Y, test_X, test_Y"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ThfTQM6YKrs","colab_type":"text"},"source":["### Import Packages\n"]},{"cell_type":"code","metadata":{"id":"6EXkkQMwYDm-","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596450450468,"user_tz":-330,"elapsed":782,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import sklearn\n","import sklearn.datasets\n","import scipy.io\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aX7lcVHHYq-W","colab_type":"text"},"source":["### 1 - Non Regularized Model"]},{"cell_type":"code","metadata":{"id":"AoZfJwxUY4dl","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1596450364945,"user_tz":-330,"elapsed":2898,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):\n","  grads = {}\n","  costs = []\n","  m = X.shape[1]\n","  layers_dims = [X.shape[0], 20, 3, 1]\n","\n","  parameters = initialize_parameters(layers_dims)\n","\n","  for i in range(0, num_iterations):\n","    if keep_prob == 1:\n","      a3, cache = forward_propagation(X, parameters)\n","    elif keep_prob < 1:\n","      a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n","    \n","    #cost function\n","    if lambd == 0:\n","      cost = compute_cost(a3, Y)\n","    else:\n","      cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n","    \n","    assert(lambd==0 or keep_prob == 1)\n","\n","    if lambd == 0 and keep_prob == 1:\n","      grads = backward_propagation(X, Y, cache)\n","    elif lambd != 0:\n","      grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n","    elif keep_prob < 1:\n","      grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n","        \n","        # Update parameters.\n","    parameters = update_parameters(parameters, grads, learning_rate)\n","        \n","        # Print the loss every 10000 iterations\n","    if print_cost and i % 10000 == 0:\n","      print(\"Cost after iteration {}: {}\".format(i, cost))\n","    if print_cost and i % 1000 == 0:\n","      costs.append(cost)\n","    \n","    # plot the cost\n","    plt.plot(costs)\n","    plt.ylabel('cost')\n","    plt.xlabel('iterations (x1,000)')\n","    plt.title(\"Learning rate =\" + str(learning_rate))\n","    plt.show()\n","    \n","    return parameters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CnS5XEX2a3gy","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1596450364948,"user_tz":-330,"elapsed":2888,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["parameters = model(train_X, train_Y)\n","print (\"On the training set:\")\n","predictions_train = predict(train_X, train_Y, parameters)\n","print (\"On the test set:\")\n","predictions_test = predict(test_X, test_Y, parameters)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z0rr_inNa6wo","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1596450364949,"user_tz":-330,"elapsed":2868,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["plt.title(\"Model without regularization\")\n","axes = plt.gca()\n","axes.set_xlim([-0.75,0.40])\n","axes.set_ylim([-0.75,0.65])\n","plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7t63IWDva_JG","colab_type":"text"},"source":["### L2 Regularization"]},{"cell_type":"code","metadata":{"id":"kOSH1Z6Ka9u1","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1596450364951,"user_tz":-330,"elapsed":2842,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["def compute_cost_with_regularization(A3, Y, parameters, lambd):\n","  m = Y.shape[1]\n","  W1 = parameters[\"W1\"]\n","  W2 = parameters[\"W2\"]\n","  W3 = parameters[\"W3\"]\n","\n","  cross_entropy_cost = compute_cost(A3, Y)\n","\n","  L2_regularization_cost = lambd * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3))) / (2 * m)\n","\n","  cost = cross_entropy_cost + L2_regularization_cost\n","\n","  return cost"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DpxS15PecDji","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1596450364953,"user_tz":-330,"elapsed":2831,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["print(\"cost = \" + str(compute_cost_with_regularization(A3, Y_assess, parameters, lambd = 0.1)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DF11lyWKcHbv","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1596450364955,"user_tz":-330,"elapsed":2826,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["def backward_propagation_with_regularization(X, Y, cache, lambd):\n","  m = X.shape[1]\n","  (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n","\n","  dZ3 = A3 - Y\n","\n","  dW3 = 1./m * np.dot(dZ3, A2.T) + (lambd * W3) / m\n","\n","  db3 = 1./m * np.sum(dZ3, axis = 1, keepdims = True)\n","\n","\n","  dA2 = np.dot(W3.T, dZ3)\n","  dZ2 = np.multiply(dA2 , np.int64(A2 > 0))\n","\n","  dW2 = 1./m * np.dot(dZ2, A1.T) + (lambd * W2) / m\n","\n","  db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n","\n","  dA1 = np.dot(W2.T, dZ2)\n","  dZ2 = np.multiply(dA1, np.int64(A1 > 0))\n","\n","  dW1 = 1./m * np.dot(dZ1, X.T) + (lambd * W1) / m\n","  db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n","\n","  gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n","                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n","                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n","  return gradients\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L6ysO0IFeSv9","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1596450364957,"user_tz":-330,"elapsed":2810,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["grads = backward_propagation_with_regularization(X_assess, Y_assess, cache, lambd = 0.7)\n","print (\"dW1 = \\n\"+ str(grads[\"dW1\"]))\n","print (\"dW2 = \\n\"+ str(grads[\"dW2\"]))\n","print (\"dW3 = \\n\"+ str(grads[\"dW3\"]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gRNL4qdKeTsL","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1596450364958,"user_tz":-330,"elapsed":2802,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["parameters = model(train_X, train_Y, lambd = 0.7)\n","print (\"On the train set:\")\n","predictions_train = predict(train_X, train_Y, parameters)\n","print (\"On the test set:\")\n","predictions_test = predict(test_X, test_Y, parameters)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mSGjkV6FeWl9","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1596450364960,"user_tz":-330,"elapsed":2786,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["plt.title(\"Model with L2-regularization\")\n","axes = plt.gca()\n","axes.set_xlim([-0.75,0.40])\n","axes.set_ylim([-0.75,0.65])\n","plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DxQQcJQceeGn","colab_type":"text"},"source":["## Drop out Regularization"]},{"cell_type":"code","metadata":{"id":"9wwEg9w3eY3j","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1596450364962,"user_tz":-330,"elapsed":2766,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):\n","  np.random.seed(1)\n","  W1 = parameters[\"W1\"]\n","  b1 = parameters[\"b1\"]\n","  W2 = parameters[\"W2\"]\n","  b2 = parameters[\"b2\"]\n","  W3 = parameters[\"W3\"]\n","  b3 = parameters[\"b3\"]\n","\n","  # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n","  Z1 = np.dot(W1, X) + b1\n","  A1 = relu(Z1)\n","\n","  D1 = np.random.rand(A1.shape[0], A1.shape[1])\n","  D1 = D1 < keep_prob\n","  A1 = A1 * D1\n","  A1 = A1 / keep_prob\n","  Z2 = np.dot(W2, A1) + b2\n","  A2 = relu(Z2)\n","\n","  D2 = np.random.rand(A2.shape[0], A2.shape[1])\n","  D2 = D2 < keep_prob\n","  A2 = A2 * D2\n","  A2 = A2 / keep_prob\n","\n","  Z3 = np.dot(W3, A2) + b3\n","  A3 = sigmoid(Z3)\n","\n","  cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n","\n","  return A3, cache\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ld-TZjm_0lSl","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1596450364963,"user_tz":-330,"elapsed":2750,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["A3, cache = forward_propagation_with_dropout(X_assess, parameters, keep_prob = 0.7)\n","print (\"A3 = \" + str(A3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1CIgLb-o0qxm","colab_type":"text"},"source":["### Backward Propagation with Dropout"]},{"cell_type":"code","metadata":{"id":"FsCSA5NX0o8s","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1596450364965,"user_tz":-330,"elapsed":2743,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["def backward_propagation_with_droput(X, Y, cache, keep_prob):\n","\n","  m = X.shape[1]\n","  (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n","\n","  dZ3 = A3 - Y\n","  dW3 = A3 - Y\n","  dW3 = 1./m * np.dot(dZ3, A2.T)\n","  db3 = 1./m * np.sum(dZ3, axis = 1, keepdims = True)\n","  dA2 = np.dot(W3.T, dZ3)\n","\n","  dA2 = dA2 * D2\n","  dA2 = dA2 / keep_prob\n","\n","  dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n","  dW2 = 1./m * np.dot(dZ2, A1.T)\n","  db2 = 1./m * np.sum(dZ2, axis = 1, keep_dims = True)\n","\n","  dA1 = np.dot(W2.T, dZ2)\n","  dA1 = dA1 * D1\n","  dA1 = dA1 / keep_prob\n","\n","  dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n","  dW1 = 1./m * np.dot(dZ1, X.T)\n","  db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n","\n","  gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n","                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n","                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n","\n","  return gradients\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-DsJT0P02lXI","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1596450364967,"user_tz":-330,"elapsed":2730,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["gradients = backward_propagation_with_dropout(X_assess, Y_assess, cache, keep_prob = 0.8)\n","\n","print (\"dA1 = \\n\" + str(gradients[\"dA1\"]))\n","print (\"dA2 = \\n\" + str(gradients[\"dA2\"]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BNfF1oeb2oMW","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1596450364968,"user_tz":-330,"elapsed":2716,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["parameters = model(train_X, train_Y, keep_prob = 0.86, learning_rate = 0.3)\n","\n","print (\"On the train set:\")\n","predictions_train = predict(train_X, train_Y, parameters)\n","print (\"On the test set:\")\n","predictions_test = predict(test_X, test_Y, parameters)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iMY1WTi52vCl","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1596450364970,"user_tz":-330,"elapsed":2707,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["plt.title(\"Model with dropout\")\n","axes = plt.gca()\n","axes.set_xlim([-0.75,0.40])\n","axes.set_ylim([-0.75,0.65])\n","plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"],"execution_count":null,"outputs":[]}]}