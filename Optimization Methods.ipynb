{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Optimization Methods.ipynb","provenance":[],"authorship_tag":"ABX9TyMpToce47qiA8rVtvLnrE0M"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6Klegq1vmQ0W","colab_type":"text"},"source":["### Predifined Functions\n","\n"]},{"cell_type":"code","metadata":{"id":"pKTtY0xblY8h","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1596450880534,"user_tz":-330,"elapsed":3763,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}},"outputId":"967f292d-2d4b-4a55-8436-d59318ae2f44"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import h5py\n","import scipy.io\n","import sklearn\n","import sklearn.datasets\n","\n","def sigmoid(x):\n","    \"\"\"\n","    Compute the sigmoid of x\n","\n","    Arguments:\n","    x -- A scalar or numpy array of any size.\n","\n","    Return:\n","    s -- sigmoid(x)\n","    \"\"\"\n","    s = 1/(1+np.exp(-x))\n","    return s\n","\n","def relu(x):\n","    \"\"\"\n","    Compute the relu of x\n","\n","    Arguments:\n","    x -- A scalar or numpy array of any size.\n","\n","    Return:\n","    s -- relu(x)\n","    \"\"\"\n","    s = np.maximum(0,x)\n","    \n","    return s\n","\n","def load_params_and_grads(seed=1):\n","    np.random.seed(seed)\n","    W1 = np.random.randn(2,3)\n","    b1 = np.random.randn(2,1)\n","    W2 = np.random.randn(3,3)\n","    b2 = np.random.randn(3,1)\n","\n","    dW1 = np.random.randn(2,3)\n","    db1 = np.random.randn(2,1)\n","    dW2 = np.random.randn(3,3)\n","    db2 = np.random.randn(3,1)\n","    \n","    return W1, b1, W2, b2, dW1, db1, dW2, db2\n","\n","\n","def initialize_parameters(layer_dims):\n","    \"\"\"\n","    Arguments:\n","    layer_dims -- python array (list) containing the dimensions of each layer in our network\n","    \n","    Returns:\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n","                    W1 -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n","                    b1 -- bias vector of shape (layer_dims[l], 1)\n","                    Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n","                    bl -- bias vector of shape (1, layer_dims[l])\n","                    \n","    Tips:\n","    - For example: the layer_dims for the \"Planar Data classification model\" would have been [2,2,1]. \n","    This means W1's shape was (2,2), b1 was (1,2), W2 was (2,1) and b2 was (1,1). Now you have to generalize it!\n","    - In the for loop, use parameters['W' + str(l)] to access Wl, where l is the iterative integer.\n","    \"\"\"\n","    \n","    np.random.seed(3)\n","    parameters = {}\n","    L = len(layer_dims) # number of layers in the network\n","\n","    for l in range(1, L):\n","        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*  np.sqrt(2 / layer_dims[l-1])\n","        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n","        \n","        assert(parameters['W' + str(l)].shape == layer_dims[l], layer_dims[l-1])\n","        assert(parameters['W' + str(l)].shape == layer_dims[l], 1)\n","        \n","    return parameters\n","\n","\n","def compute_cost(a3, Y):\n","    \n","    \"\"\"\n","    Implement the cost function\n","    \n","    Arguments:\n","    a3 -- post-activation, output of forward propagation\n","    Y -- \"true\" labels vector, same shape as a3\n","    \n","    Returns:\n","    cost - value of the cost function\n","    \"\"\"\n","    m = Y.shape[1]\n","    \n","    logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)\n","    cost = 1./m * np.sum(logprobs)\n","    \n","    return cost\n","\n","def forward_propagation(X, parameters):\n","    \"\"\"\n","    Implements the forward propagation (and computes the loss) presented in Figure 2.\n","    \n","    Arguments:\n","    X -- input dataset, of shape (input size, number of examples)\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n","                    W1 -- weight matrix of shape ()\n","                    b1 -- bias vector of shape ()\n","                    W2 -- weight matrix of shape ()\n","                    b2 -- bias vector of shape ()\n","                    W3 -- weight matrix of shape ()\n","                    b3 -- bias vector of shape ()\n","    \n","    Returns:\n","    loss -- the loss function (vanilla logistic loss)\n","    \"\"\"\n","    \n","    # retrieve parameters\n","    W1 = parameters[\"W1\"]\n","    b1 = parameters[\"b1\"]\n","    W2 = parameters[\"W2\"]\n","    b2 = parameters[\"b2\"]\n","    W3 = parameters[\"W3\"]\n","    b3 = parameters[\"b3\"]\n","    \n","    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n","    z1 = np.dot(W1, X) + b1\n","    a1 = relu(z1)\n","    z2 = np.dot(W2, a1) + b2\n","    a2 = relu(z2)\n","    z3 = np.dot(W3, a2) + b3\n","    a3 = sigmoid(z3)\n","    \n","    cache = (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3)\n","    \n","    return a3, cache\n","\n","def backward_propagation(X, Y, cache):\n","    \"\"\"\n","    Implement the backward propagation presented in figure 2.\n","    \n","    Arguments:\n","    X -- input dataset, of shape (input size, number of examples)\n","    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat)\n","    cache -- cache output from forward_propagation()\n","    \n","    Returns:\n","    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n","    \"\"\"\n","    m = X.shape[1]\n","    (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3) = cache\n","    \n","    dz3 = 1./m * (a3 - Y)\n","    dW3 = np.dot(dz3, a2.T)\n","    db3 = np.sum(dz3, axis=1, keepdims = True)\n","    \n","    da2 = np.dot(W3.T, dz3)\n","    dz2 = np.multiply(da2, np.int64(a2 > 0))\n","    dW2 = np.dot(dz2, a1.T)\n","    db2 = np.sum(dz2, axis=1, keepdims = True)\n","    \n","    da1 = np.dot(W2.T, dz2)\n","    dz1 = np.multiply(da1, np.int64(a1 > 0))\n","    dW1 = np.dot(dz1, X.T)\n","    db1 = np.sum(dz1, axis=1, keepdims = True)\n","    \n","    gradients = {\"dz3\": dz3, \"dW3\": dW3, \"db3\": db3,\n","                 \"da2\": da2, \"dz2\": dz2, \"dW2\": dW2, \"db2\": db2,\n","                 \"da1\": da1, \"dz1\": dz1, \"dW1\": dW1, \"db1\": db1}\n","    \n","    return gradients\n","\n","def predict(X, y, parameters):\n","    \"\"\"\n","    This function is used to predict the results of a  n-layer neural network.\n","    \n","    Arguments:\n","    X -- data set of examples you would like to label\n","    parameters -- parameters of the trained model\n","    \n","    Returns:\n","    p -- predictions for the given dataset X\n","    \"\"\"\n","    \n","    m = X.shape[1]\n","    p = np.zeros((1,m), dtype = np.int)\n","    \n","    # Forward propagation\n","    a3, caches = forward_propagation(X, parameters)\n","    \n","    # convert probas to 0/1 predictions\n","    for i in range(0, a3.shape[1]):\n","        if a3[0,i] > 0.5:\n","            p[0,i] = 1\n","        else:\n","            p[0,i] = 0\n","\n","    # print results\n","\n","    #print (\"predictions: \" + str(p[0,:]))\n","    #print (\"true labels: \" + str(y[0,:]))\n","    print(\"Accuracy: \"  + str(np.mean((p[0,:] == y[0,:]))))\n","    \n","    return p\n","\n","def load_2D_dataset():\n","    data = scipy.io.loadmat('datasets/data.mat')\n","    train_X = data['X'].T\n","    train_Y = data['y'].T\n","    test_X = data['Xval'].T\n","    test_Y = data['yval'].T\n","\n","    plt.scatter(train_X[0, :], train_X[1, :], c=train_Y, s=40, cmap=plt.cm.Spectral);\n","    \n","    return train_X, train_Y, test_X, test_Y\n","\n","def plot_decision_boundary(model, X, y):\n","    # Set min and max values and give it some padding\n","    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n","    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n","    h = 0.01\n","    # Generate a grid of points with distance h between them\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","    # Predict the function value for the whole grid\n","    Z = model(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","    # Plot the contour and training examples\n","    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n","    plt.ylabel('x2')\n","    plt.xlabel('x1')\n","    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n","    plt.show()\n","    \n","def predict_dec(parameters, X):\n","    \"\"\"\n","    Used for plotting decision boundary.\n","    \n","    Arguments:\n","    parameters -- python dictionary containing your parameters \n","    X -- input data of size (m, K)\n","    \n","    Returns\n","    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n","    \"\"\"\n","    \n","    # Predict using forward propagation and a classification threshold of 0.5\n","    a3, cache = forward_propagation(X, parameters)\n","    predictions = (a3 > 0.5)\n","    return predictions\n","\n","def load_dataset():\n","    np.random.seed(3)\n","    train_X, train_Y = sklearn.datasets.make_moons(n_samples=300, noise=.2) #300 #0.2 \n","    # Visualize the data\n","    plt.scatter(train_X[:, 0], train_X[:, 1], c=train_Y, s=40, cmap=plt.cm.Spectral);\n","    train_X = train_X.T\n","    train_Y = train_Y.reshape((1, train_Y.shape[0]))\n","    \n","    return train_X, train_Y"],"execution_count":1,"outputs":[{"output_type":"stream","text":["<ipython-input-1-05daa9668d6f>:76: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n","  assert(parameters['W' + str(l)].shape == layer_dims[l], layer_dims[l-1])\n","<ipython-input-1-05daa9668d6f>:77: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n","  assert(parameters['W' + str(l)].shape == layer_dims[l], 1)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"vQHHHST3os5y","colab_type":"text"},"source":["## Test Cases\n"]},{"cell_type":"code","metadata":{"id":"iZvctYJooq3K","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596450893461,"user_tz":-330,"elapsed":1748,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["import numpy as np\n","\n","def update_parameters_with_gd_test_case():\n","    np.random.seed(1)\n","    learning_rate = 0.01\n","    W1 = np.random.randn(2,3)\n","    b1 = np.random.randn(2,1)\n","    W2 = np.random.randn(3,3)\n","    b2 = np.random.randn(3,1)\n","\n","    dW1 = np.random.randn(2,3)\n","    db1 = np.random.randn(2,1)\n","    dW2 = np.random.randn(3,3)\n","    db2 = np.random.randn(3,1)\n","    \n","    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n","    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n","    \n","    return parameters, grads, learning_rate\n","\n","\n","def random_mini_batches_test_case():\n","    np.random.seed(1)\n","    mini_batch_size = 64\n","    X = np.random.randn(12288, 148)\n","    Y = np.random.randn(1, 148) < 0.5\n","    return X, Y, mini_batch_size\n","\n","def initialize_velocity_test_case():\n","    np.random.seed(1)\n","    W1 = np.random.randn(2,3)\n","    b1 = np.random.randn(2,1)\n","    W2 = np.random.randn(3,3)\n","    b2 = np.random.randn(3,1)\n","    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n","    return parameters\n","\n","def update_parameters_with_momentum_test_case():\n","    np.random.seed(1)\n","    W1 = np.random.randn(2,3)\n","    b1 = np.random.randn(2,1)\n","    W2 = np.random.randn(3,3)\n","    b2 = np.random.randn(3,1)\n","\n","    dW1 = np.random.randn(2,3)\n","    db1 = np.random.randn(2,1)\n","    dW2 = np.random.randn(3,3)\n","    db2 = np.random.randn(3,1)\n","    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n","    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n","    v = {'dW1': np.array([[ 0.,  0.,  0.],\n","        [ 0.,  0.,  0.]]), 'dW2': np.array([[ 0.,  0.,  0.],\n","        [ 0.,  0.,  0.],\n","        [ 0.,  0.,  0.]]), 'db1': np.array([[ 0.],\n","        [ 0.]]), 'db2': np.array([[ 0.],\n","        [ 0.],\n","        [ 0.]])}\n","    return parameters, grads, v\n","    \n","def initialize_adam_test_case():\n","    np.random.seed(1)\n","    W1 = np.random.randn(2,3)\n","    b1 = np.random.randn(2,1)\n","    W2 = np.random.randn(3,3)\n","    b2 = np.random.randn(3,1)\n","    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n","    return parameters\n","\n","def update_parameters_with_adam_test_case():\n","    np.random.seed(1)\n","    v, s = ({'dW1': np.array([[ 0.,  0.,  0.],\n","         [ 0.,  0.,  0.]]), 'dW2': np.array([[ 0.,  0.,  0.],\n","         [ 0.,  0.,  0.],\n","         [ 0.,  0.,  0.]]), 'db1': np.array([[ 0.],\n","         [ 0.]]), 'db2': np.array([[ 0.],\n","         [ 0.],\n","         [ 0.]])}, {'dW1': np.array([[ 0.,  0.,  0.],\n","         [ 0.,  0.,  0.]]), 'dW2': np.array([[ 0.,  0.,  0.],\n","         [ 0.,  0.,  0.],\n","         [ 0.,  0.,  0.]]), 'db1': np.array([[ 0.],\n","         [ 0.]]), 'db2': np.array([[ 0.],\n","         [ 0.],\n","         [ 0.]])})\n","    W1 = np.random.randn(2,3)\n","    b1 = np.random.randn(2,1)\n","    W2 = np.random.randn(3,3)\n","    b2 = np.random.randn(3,1)\n","\n","    dW1 = np.random.randn(2,3)\n","    db1 = np.random.randn(2,1)\n","    dW2 = np.random.randn(3,3)\n","    db2 = np.random.randn(3,1)\n","    \n","    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n","    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n","    \n","    return parameters, grads, v, s\n","    "],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"aoE5LfeOmmJq","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596450900210,"user_tz":-330,"elapsed":1994,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.io\n","import math\n","import sklearn\n","import sklearn.datasets\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TBDyRdQ_nAOl","colab_type":"text"},"source":["##  Gradient Descent\n"]},{"cell_type":"code","metadata":{"id":"cD41OEVpnGUV","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596450905040,"user_tz":-330,"elapsed":2320,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["def update_parameters_with_gd(parameters, grads, learning_rate):\n","   \n","    L = len(parameters) // 2 # number of layers in the neural networks\n","\n","    for l in range(L):\n","        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n","        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n","        \n","    return parameters"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"how8rPZvpW4B","colab_type":"text"},"source":["### Random Mini batches\n"]},{"cell_type":"code","metadata":{"id":"7c0sEBJmpcJj","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596450926027,"user_tz":-330,"elapsed":1549,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["def random_min_batches(X, Y, mini_batch_size = 64, seed = 0):\n","  np.random.seed(seed)\n","  m = X.shape[1]\n","  mini_batches = []\n","\n","  permutation = list(np.random.permutation(m))\n","  shuffled_X = X[:, permutation]\n","  shuffled_Y = Y[:, permutation].reshape((1,m))\n","\n","  num_complete_minibatches = math.floor(m /  mini_batch_size)\n","  for k in range(0, num_complete_minibatches):\n","    mini_batch_X = shuffled_X[:,k * mini_batch_size:(k + 1) * mini_batch_size]\n","    mini_batch_Y = shuffled_Y[:,k * mini_batch_size:(k + 1) * mini_batch_size]\n","\n","    mini_batch = (mini_batch_X, mini_batch_Y)\n","    mini_batches.append(mini_batch)\n","\n","    if m % mini_batch_size != 0:\n","      end = m - mini_batch_size * math.floor(m / mini_batch_size)\n","      mini_batch_X = shuffled_X[:,num_complete_minibatches * mini_batch_size:]\n","      mini_batch_Y = shuffled_Y[:,num_complete_minibatches * mini_batch_size:]\n","        \n","      mini_batch = (mini_batch_X, mini_batch_Y)\n","      mini_batches.append(mini_batch)\n","    \n","    return mini_batches\n","    \n","\n","  \n"," "],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V8umu-FWTfcr","colab_type":"text"},"source":["## Momentum"]},{"cell_type":"code","metadata":{"id":"rTxLSY7dThbr","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596450931610,"user_tz":-330,"elapsed":2475,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["def initialize_velocity(parameters):\n","\n","  L = len(parameters) // 2\n","  v = {}\n","\n","  for l in range(L):\n","    v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l+1)])\n","    v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l+1)])\n","\n","    return v"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"XEYeePz6T_XM","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596450932864,"user_tz":-330,"elapsed":1310,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n","  L = len(parameters) // 2\n","\n","  for l in range(L):\n","    v[\"dW\" + str(l + 1)] = beta * v[\"dW\" + str(l + 1)] + (1 - beta) * grads['dW' + str(l + 1)]\n","    v[\"db\" + str(l + 1)] = beta * v[\"db\" + str(l + 1)] + (1 - beta) * grads['db' + str(l + 1)]\n","\n","    parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v[\"dW\" + str(l + 1)]\n","    parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v[\"db\" + str(l + 1)]\n","\n","  return parameters, v\n","  "],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oulGOtrnUlgc","colab_type":"text"},"source":["## Adam\n","\n","Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp and Momentum."]},{"cell_type":"code","metadata":{"id":"5YyGUtjlUoc8","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596450937104,"user_tz":-330,"elapsed":1877,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["def initialize_adam(parameters):\n","  L = len(parameters) // 2\n","  v = {}\n","  s = {}\n","\n","  for l in range(L):\n","    v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n","    v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n","\n","    s[\"dW\" + str(l+1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n","    s[\"db\" + str(l+1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n","\n","    return v, s"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"E0kdjrHhVO2T","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596450939857,"user_tz":-330,"elapsed":1577,"user":{"displayName":"Saurabh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBC_TqONQ_b1COxua5zOunXaAKYuQBoL8SNELZjQ=s64","userId":"05893218404871049262"}}},"source":["def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n","                                beta1 = 0.9, beta2= 0.999, epsilon = 1e-8):\n","  L = len(parameters) // 2\n","  v_corrected = {}\n","  s_corrected = {}\n","\n","  for l in range(L):\n","    v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads['dW' + str(l + 1)]\n","    v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads['db' + str(l + 1)]\n","\n","    v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - np.power(beta1, t))\n","    v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - np.power(beta1, t))\n","\n","    s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grads['dW' + str(l + 1)], 2)\n","    s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grads['db' + str(l + 1)], 2)\n","\n","    s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - np.power(beta2, t))\n","    s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - np.power(beta2, t))\n","\n","    parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v_corrected[\"dW\" + str(l + 1)] / np.sqrt(s_corrected[\"dW\" + str(l + 1)] + epsilon)\n","    parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v_corrected[\"db\" + str(l + 1)] / np.sqrt(s_corrected[\"db\" + str(l + 1)] + epsilon)\n","\n","  return parameters, v, s\n","  \n","        "],"execution_count":10,"outputs":[]}]}